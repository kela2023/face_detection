{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc927f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "facedetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "video=cv2.VideoCapture(0)\n",
    "video.set(3, 640)\n",
    "video.set(4, 480)\n",
    "font=cv2.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "\n",
    "model = load_model('keras_model.h5')\n",
    "model_genre = load_model('./model-013.model')\n",
    "classifier = load_model('model.h5')\n",
    "\n",
    "emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']\n",
    "#genre\n",
    "labels_dict={0:'Male',1:'Female'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "#classification de visages\n",
    "def get_className(classNo):\n",
    "\tif classNo==0:\n",
    "\t\treturn \"Silue\"\n",
    "\telif classNo==1:\n",
    "\t\treturn \"Kela\"\n",
    "\n",
    "\n",
    "faceDetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "class Videeo(object):\n",
    "    def __init__(self):\n",
    "        self.video=cv2.VideoCapture(0)\n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "    def get_frame(self):\n",
    "        ret,frame=self.video.read()\n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        faces=faceDetect.detectMultiScale(gray,1,3,5)\n",
    "        for x,y,w,h in faces:\n",
    "            x1,y1=x+w, y+h\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,255),1)\n",
    "            cv2.line(frame,(x,y),(x+30,y),(255,0,255),6)\n",
    "            cv2.line(frame,(x,y),(x,+30),(255,0,255,6))\n",
    "\n",
    "            cv2.line(frame,(x1,y),(x1-30,y),(255,0,255),6)\n",
    "            cv2.line(frame,(x1,y),(x1,y+30),(255,0,255),6)\n",
    "\n",
    "            cv2.line(frame,(x,y),(x+30,y1),(255,0,255),6)\n",
    "            cv2.line(frame,(x,y1),(x,y1-30),(255,0,255),6)\n",
    "\n",
    "            cv2.line(frame,(x1,y1),(x1-30,y1),(255,0,255),6)\n",
    "            cv2.line(frame,(x1,y1),(x1,y1-30),(255,0,255),6)\n",
    "            \n",
    "            crop_img=imgOrignal[y:y+h,x:x+h]\n",
    "            img=cv2.resize(crop_img, (224,224))\n",
    "            img=img.reshape(1, 224, 224, 3)\n",
    "            prediction=model.predict(img)\n",
    "            classIndex=np.argmax(prediction)\n",
    "            probabilityValue=np.amax(prediction)\n",
    "\n",
    "            if classIndex==0:\n",
    "                cv2.rectangle(imgOrignal,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "                cv2.rectangle(imgOrignal, (x,y-40),(x+w, y), (0,255,0),-2)\n",
    "                cv2.putText(imgOrignal, str(get_className(classIndex)),(x,y-10), font, 0.75, (255,255,255),1, cv2.LINE_AA)\n",
    "            elif classIndex==1:\n",
    "                cv2.rectangle(imgOrignal,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "                cv2.rectangle(imgOrignal, (x,y-40),(x+w, y), (0,0,255),-2)\n",
    "                cv2.putText(imgOrignal, str(get_className(classIndex)),(x,y-10), font, 0.75, (255,255,255),1, cv2.LINE_AA)\n",
    "\n",
    "        #   \tcv2.putText(imgOrignal,str(round(probabilityValue*100, 2))+\"%\" ,(180, 75), font, 0.75, (255,0,0),2, cv2.LINE_AA)\n",
    "                face_img=gray[y:y+w,x:x+w]#begin\n",
    "                resized=cv2.resize(face_img,(32,32))\n",
    "                normalized=resized/255.0\n",
    "                reshaped=np.reshape(normalized,(1,32,32,1))\n",
    "                result=model_genre.predict(reshaped)\n",
    "                label=np.argmax(result,axis=1)[0]\n",
    "                label_fr=np.argmax(result,axis=1)[0]\n",
    "                cv2.putText(imgOrignal, labels_dict[label], (x, y-40),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        #end\n",
    "        #ella\n",
    "                roi_gray = gray[y:y+h,x:x+w]\n",
    "                roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "                if np.sum([roi_gray])!=0:\n",
    "                    roi = roi_gray.astype('float')/255.0\n",
    "                    roi = img_to_array(roi)\n",
    "                    roi = np.expand_dims(roi,axis=0)\n",
    "                    prediction = classifier.predict(roi)[0]\n",
    "                    label=emotion_labels[prediction.argmax()]\n",
    "                    label_position = (x,y)\n",
    "                    cv2.putText(imgOrignal,label,(x,y-80),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "                else:\n",
    "                    cv2.putText(imgOrignal,'No Faces',(x,y-110),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "\n",
    "        \n",
    "        ret,jpg=cv2.imencode('.jpg',frame)\n",
    "        return jpg.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff30b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
